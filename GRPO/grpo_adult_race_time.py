import torch
import torch.nn as nn
from models import AdultCensusModel
from torch.distributions import Normal
import numpy as np
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
from Environment import NeuronScalingEnv
from utils import compute_metrics, convert_to_builtin_types, preprocess_adult_census
from KeyNeuronsIdentification import KeyNeuronsIdentification
import json
import os

device = "cpu"


class PolicyNet(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_dim=256, scale_bounds=(-1.0, 1.0)):
        super(PolicyNet, self).__init__()
        self.fc1 = nn.Linear(obs_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.fc_mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Parameter(torch.zeros(1, action_dim))

        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()

        self.scale_bounds = scale_bounds

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        mean = self.tanh(self.fc_mean(x))
        return mean

    def get_distribution(self, obs):
        mean = self.forward(obs)
        log_std = self.log_std.expand_as(mean)
        std = torch.exp(log_std)
        return Normal(mean, std)

    def select_action(self, obs):
        with torch.no_grad():
            dist = self.get_distribution(obs)
            action = dist.sample()
            action = torch.clamp(action, self.scale_bounds[0], self.scale_bounds[1])
        return action, dist.log_prob(action).sum(dim=-1)


def collect_trajectory(env, policy_net, device="cpu"):
    """
    ä»å•ä¸ªç¯å¢ƒé‡‡æ ·ä¸€æ¡è½¨è¿¹ï¼ˆè¿™ä¸ªè½¨è¿¹å°±æ˜¯å•æ­¥ç¼©æ”¾ï¼‰
    env: ç¥ç»å…ƒç¼©æ”¾ç¯å¢ƒ
    policy_net: ç­–ç•¥ç½‘ç»œ

    è¿”å›ï¼šstate, action, log_prob, reward
    """
    state = env.reset()  # é‡ç½®ç¯å¢ƒ

    # å°†çŠ¶æ€è½¬æ¢ä¸ºå¼ é‡
    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)

    # é€‰æ‹©åŠ¨ä½œ
    action_tensor, log_prob = policy_net.select_action(state_tensor)
    action = action_tensor.detach().cpu().numpy()[0]

    # æ‰§è¡ŒåŠ¨ä½œ
    next_state, reward, done, info = env.step(action)

    trajectory = {
        "state": state,
        "action": action,
        "log_prob": log_prob.item(),
        "reward": reward,
        "info": info
    }

    return trajectory


def collect_batch_trajectories(env, policy_net, batch_size=10, device="cpu"):
    """
    æ”¶é›†å¤šæ¡è½¨è¿¹ä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡
    """
    batch_trajectories = {
        "states": [],
        "actions": [],
        "log_probs": [],
        "rewards": []
    }

    fairness_list = []
    performance_list = []
    reward_list = []

    for _ in range(batch_size):
        trajectory = collect_trajectory(env, policy_net, device)

        batch_trajectories["states"].append(trajectory["state"])
        batch_trajectories["actions"].append(trajectory["action"])
        batch_trajectories["log_probs"].append(trajectory["log_prob"])
        batch_trajectories["rewards"].append(trajectory["reward"])

        fairness_list.append(trajectory["info"]["fairness"])
        performance_list.append(trajectory["info"]["performance"])
        reward_list.append(trajectory["reward"])

    # å°†åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„
    batch_trajectories["states"] = np.array(batch_trajectories["states"])
    batch_trajectories["actions"] = np.array(batch_trajectories["actions"])
    batch_trajectories["log_probs"] = np.array(batch_trajectories["log_probs"])
    batch_trajectories["rewards"] = np.array(batch_trajectories["rewards"])

    # è®¡ç®—ä¿¡æ¯çš„å¹³å‡å€¼
    avg_fairness = np.mean(fairness_list)
    avg_performance = np.mean(performance_list)
    avg_reward = np.mean(reward_list)

    info = {
        "avg_fairness": avg_fairness,
        "avg_performance": avg_performance,
        "best_fairness": env.best_fairness,
        "best_performance": env.best_performance,
        "best_scales": env.best_scales
    }

    return batch_trajectories, avg_reward, info


def calc_advantages_with_grpo(trajectories):
    """ä»è½¨è¿¹ä¸­æå–å¥–åŠ±ï¼Œå¹¶æ ‡å‡†åŒ–"""
    rewards = trajectories["rewards"]  # æå–å¥–åŠ±
    mean_reward = np.mean(rewards)  # è®¡ç®—å¹³å‡å€¼
    std_reward = np.std(rewards) + 1e-8  # è®¡ç®—æ ‡å‡†å·®ï¼ˆ1e-8æ˜¯é˜²æ­¢0é™¤ï¼‰
    advantages = (rewards - mean_reward) / std_reward  # æ ‡å‡†åŒ–

    return advantages


def grpo_update(trajectories, policy_net, optimizer, device="cpu", n_iterations=20, eps=0.2):
    # è®¡ç®—æ ‡å‡†åŒ–åçš„ä¼˜åŠ¿
    advantages = calc_advantages_with_grpo(trajectories)

    # å°†æ•°æ®è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    states = torch.tensor(trajectories["states"], dtype=torch.float32, device=device)
    actions = torch.tensor(trajectories["actions"], dtype=torch.float32, device=device)
    old_log_probs = torch.tensor(trajectories["log_probs"], dtype=torch.float32, device=device)
    advantages = torch.tensor(advantages, dtype=torch.float32, device=device).unsqueeze(-1)

    batch_size = len(states)

    # æ‰§è¡Œn_iterationsæ¬¡æ›´æ–°
    for _ in range(n_iterations):
        loss = 0

        for i in range(batch_size):
            state = states[i].unsqueeze(0)
            action = actions[i].unsqueeze(0)
            old_log_prob = old_log_probs[i]
            advantage = advantages[i]

            # é‡æ–°è¯„ä¼°åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            dist = policy_net.get_distribution(state)
            new_log_prob = dist.log_prob(action).sum(dim=-1)

            # è®¡ç®—æ¦‚ç‡æ¯”
            ratio = torch.exp(new_log_prob - old_log_prob)

            # è®¡ç®—ä¸¤ä¸ªsurrogateé¡¹
            surr1 = ratio * advantage
            surr2 = torch.clamp(ratio, 1 - eps, 1 + eps) * advantage

            # è®¡ç®—æŸå¤±
            trajectory_loss = -torch.min(surr1, surr2)
            loss += trajectory_loss

        # ç”¨batch_sizeå½’ä¸€åŒ–æŸå¤±
        loss /= batch_size

        # æ›´æ–°ç­–ç•¥ç½‘ç»œ
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return loss.item()


def train_grpo(env, max_time_minutes=5, batch_size=10, lr=0.002, n_iterations=10, eps=0.2):
    """
    ä½¿ç”¨GRPOè®­ç»ƒç¥ç»å…ƒç¼©æ”¾ç¯å¢ƒ

    å‚æ•°:
    env: ç¥ç»å…ƒç¼©æ”¾ç¯å¢ƒ
    max_episodes: æœ€å¤§è®­ç»ƒè½®æ•°
    batch_size: æ¯æ‰¹æ¬¡é‡‡æ ·çš„è½¨è¿¹æ•°
    lr: å­¦ä¹ ç‡
    n_iterations: æ¯æ‰¹æ¬¡çš„æ›´æ–°è¿­ä»£æ¬¡æ•°
    eps: GRPOçš„æˆªæ–­å‚æ•°
    performance_threshold: æ€§èƒ½é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„è§£å†³æ–¹æ¡ˆä¸ä¼šè¢«è€ƒè™‘
    early_stop_fairness: å¦‚æœè¾¾åˆ°æ­¤å…¬å¹³æ€§æŒ‡æ ‡ï¼Œåˆ™æå‰åœæ­¢è®­ç»ƒ

    è¿”å›:
    history: è®­ç»ƒå†å²è®°å½•
    """
    # è·å–è§‚æµ‹å’ŒåŠ¨ä½œç»´åº¦
    obs_dim = len(env.reset())
    action_dim = len(env.candidate_neurons)
    hidden_dim = action_dim

    # åˆ›å»ºç­–ç•¥ç½‘ç»œå’Œä¼˜åŒ–å™¨
    policy_net = PolicyNet(obs_dim, action_dim, hidden_dim).to(device)
    optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)

    # è®°å½•è®­ç»ƒå†å²
    history = {
        "episode_rewards": [],
        "fairness": [],
        "performance": [],
        "best_fairness": [],
        "best_performance": [],
        "episodes_completed": 0
    }

    start_time = time.time()
    end_time = start_time + max_time_minutes * 60
    episode = 0

    # ä¸»è®­ç»ƒå¾ªç¯
    with tqdm() as pbar:
        while time.time() < end_time:
            # æ”¶é›†ä¸€æ‰¹è½¨è¿¹
            trajectories, avg_reward, info = collect_batch_trajectories(env, policy_net, batch_size, device)

            # ä½¿ç”¨GRPOæ›´æ–°ç­–ç•¥
            loss = grpo_update(trajectories, policy_net, optimizer, device, n_iterations, eps)

            # è®°å½•å†å²
            history["episode_rewards"].append(avg_reward)
            history["fairness"].append(info["avg_fairness"])
            history["performance"].append(info["avg_performance"])
            history["best_fairness"].append(info["best_fairness"])
            history["best_performance"].append(info["best_performance"])
            history["episodes_completed"] += 1

            remaining_time = max(0, end_time - time.time())
            pbar.set_description(
                f"Episode {episode}, Remaining: {remaining_time:.1f}s")
            pbar.update(1)
            episode += 1

            # æ‰“å°ä¿¡æ¯
            if episode % 10 == 0 or episode == max_episodes - 1:
                print(f"Episode {episode}, Avg Reward: {avg_reward:.4f}, Fairness: {info['avg_fairness']:.4f}, "
                      f"Performance: {info['avg_performance']:.4f}, Best Fairness: {info['best_fairness']:.4f}, "
                      f"Best Performance: {info['best_performance']:.4f}")

    # æ‰“å°æœ€ä½³ç»“æœ
    print("\næœ€ä½³ç»“æœ:")
    print(f"å…¬å¹³æ€§: {env.best_fairness:.4f}")
    print(f"æ€§èƒ½: {env.best_performance:.4f}")
    print(f"æœ€ä½³ç¼©æ”¾å› å­: {env.best_scales}")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(history)

    return env.best_fairness, env.best_performance, env.best_scales, history["episodes_completed"] * batch_size


def plot_training_curves(history):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    # å¥–åŠ±æ›²çº¿
    axs[0, 0].plot(history["episode_rewards"])
    axs[0, 0].set_title("avg_rewards")
    axs[0, 0].set_xlabel("epoch")
    axs[0, 0].set_ylabel("reward")
    axs[0, 0].grid(True)

    # å…¬å¹³æ€§æ›²çº¿
    axs[0, 1].plot(history["fairness"], label="current fairness")
    axs[0, 1].plot(history["best_fairness"], label="best fairness", linestyle="--")
    axs[0, 1].set_title("fairness metric")
    axs[0, 1].set_xlabel("epoch")
    axs[0, 1].set_ylabel("fairness")
    axs[0, 1].legend()
    axs[0, 1].grid(True)

    # æ€§èƒ½æ›²çº¿
    axs[1, 0].plot(history["performance"], label="current performance")
    axs[1, 0].plot(history["best_performance"], label="best performance", linestyle="--")
    axs[1, 0].set_title("performance metric")
    axs[1, 0].set_xlabel("epoch")
    axs[1, 0].set_ylabel("performance")
    axs[1, 0].legend()
    axs[1, 0].grid(True)

    # å…¬å¹³æ€§ä¸æ€§èƒ½çš„æ•£ç‚¹å›¾
    axs[1, 1].scatter(history["fairness"], history["performance"], alpha=0.5)
    axs[1, 1].scatter([history["best_fairness"][-1]], [history["best_performance"][-1]],
                      color="red", s=100, marker="*", label="best")
    axs[1, 1].set_title("fairness vs performance")
    axs[1, 1].set_xlabel("fairness")
    axs[1, 1].set_ylabel("performance")
    axs[1, 1].legend()
    axs[1, 1].grid(True)

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    hidden_dims = [64, 128, 256, 256, 256, 256, 128, 64]
    key_layers_num = 4
    threshold = 0.01
    max_episodes = 200
    max_time_minutes = 5
    layers_mode = "key_layers"
    neurons_mode = "key_neurons"
    model_paths = [
       'adult-0.6901-42-ss-0.2.pt',
       'adult-0.7104-74364756-ss-0.2.pt',
       'adult-0.7035-3872938-ss-0.2.pt',
       'adult-0.6937-586933-ss-0.2.pt',
       'adult-0.6976-93276487-ss-0.2.pt',
        'adult-0.6962-6489372-ss-0.2.pt',
        'adult-0.7114-2875882-ss-0.2.pt',
        'adult-0.6999-1894375983-ss-0.2.pt',
        'adult-0.7103-89540917-ss-0.2.pt',
        'adult-0.7002-582093843-ss-0.2.pt',
    ]
    for model_path in model_paths:
        _, _, seed, _, dropout_rate = model_path[:-3].split("-")
        seed = int(seed)
        p = float(dropout_rate)
        dataset = "adult_race"
        adult_race = (23, 27)

        _, X_val, _, y_val, sens_idx = preprocess_adult_census(seed)
        sens_val = X_val[:, adult_race[0]: adult_race[1]]
        sens_classes = [0, 1]

        # num_1 = (sens_val == 1).sum().item()
        # num_0 = (sens_val == 0).sum().item()
        # total = sens_val.numel()  # ç­‰ä»·äº len(sens_val)
        #
        # # è®¡ç®—æ¯”ä¾‹
        # ratio_1 = num_1 / total
        # ratio_0 = num_0 / total
        #
        # print(f"ğŸ”¢ sens_val = 1: {num_1} ({ratio_1:.4f})")
        # print(f"ğŸ”¢ sens_val = 0: {num_0} ({ratio_0:.4f})")

        # X_val, y_val, sens_val = X_val.to(device), y_val.to(device), sens_val.to(device)

        model = AdultCensusModel(p=p).to(device)
        state_dict = torch.load(f"../saved_models/adult/{model_path}", map_location=torch.device('cpu'))
        model.load_state_dict(state_dict)
        model.eval()

        baseline_eod, baseline_dpd, baseline_di, baseline_f1, baseline_acc = compute_metrics(
            model, X_val, y_val, sens_val, sens_classes, dataset
        )
        print("\nValidation Set Baseline Metrics:")
        print(f"EOD: {baseline_eod:.4f}")
        print(f"DPD: {baseline_dpd:.4f}")
        print(f"DI: {baseline_di:.4f}")
        print(f"F1: {baseline_f1:.4f}")
        print(f"Accuracy: {baseline_acc:.4f}")

        # éœ€è¦è·å–key neurons
        hyperparameter_search_type = "RandomSearch"  # RandomSearch or GridSearch
        start_time = time.time()
        layer_sizes_accuracy, key_neurons, total_train_time = KeyNeuronsIdentification(model, hidden_dims, X_val,
        sens_idx, threshold, hyperparameter_search_type, dataset).identify()
        identification_time = time.time() - start_time
        print("Identification Time Cost:", identification_time)

        save_dict = {
            "layer_sizes_accuracy": layer_sizes_accuracy,
            "key_neurons": key_neurons,
            "baseline_fairness_eod on Val": baseline_eod,
            "baseline_fairness_dpd on Val": baseline_dpd,
            "baseline_fairness_di on Val": baseline_di,
            "baseline_performance on Val": baseline_f1,
            "baseline_accuracy on Val": baseline_acc,
        }

        save_dict = convert_to_builtin_types(save_dict)

        save_dir = f"intermediate results/Adult_race_time"
        os.makedirs(save_dir, exist_ok=True)

        save_path = os.path.join(save_dir, f"{seed}_{layers_mode}_{neurons_mode}_{max_time_minutes}_avg.json")
        with open(save_path, "w") as f:
            json.dump(save_dict, f, indent=4, separators=(',', ': '))

        layer_name_keys = list(key_neurons.keys())[:key_layers_num]

        key_neurons_final = []
        for layer in layer_name_keys:
            key_neurons_final.extend((layer, i) for i in key_neurons[layer])
        print("key_neurons_final", key_neurons_final)

        env = NeuronScalingEnv(model, key_neurons_final, baseline_eod, baseline_f1,
                               X_val, y_val, sens_val, sens_classes, dataset)

        best_fairness, best_performance, best_scales, num_iterations = train_grpo(
            env=env,
            max_time_minutes=max_time_minutes,
            batch_size=10,
            lr=0.005,
            n_iterations=10,
            eps=0.2
        )

        optimization_time = time.time() - start_time - identification_time
        print("Optimization Time Cost:", optimization_time)

        save_dict["repaired_fairness_eod on Val"] = best_fairness
        save_dict["repaired_performance on val"] = best_performance

        model.register_scaling_hooks(key_neurons_final, best_scales)
        repaired_eod, repaired_dpd, repaired_di, repaired_f1, repaired_acc = compute_metrics(
            model, X_val, y_val, sens_val, sens_classes, dataset
        )
        save_dict["repaired_fairness_dpd on val"] = repaired_dpd
        save_dict["repaired_fairness_di on val"] = repaired_di
        save_dict["repaired_accuracy on val"] = repaired_acc
        save_dict["identification time cost"] = identification_time
        save_dict["optimization time cost"] = optimization_time
        save_dict["RF train time cost"] = total_train_time
        save_dict["num_iteration"] = num_iterations
        save_dict["key_neurons_final_number"] = len(key_neurons_final)

        save_dict = convert_to_builtin_types(save_dict)

        with open(save_path, "w") as f:
            json.dump(save_dict, f, indent=4, separators=(',', ': '))
